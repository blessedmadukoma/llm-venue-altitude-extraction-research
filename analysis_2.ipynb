{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a2fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from db import init_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f87f988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing database...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conn = init_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1197ce",
   "metadata": {},
   "source": [
    "**Get the matched predictions with errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c2441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/6bp46ck12x1bnxnr0d6pcs440000gn/T/ipykernel_20837/3081177059.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    vp.canonical_venue,\n",
    "    vp.model_family,\n",
    "    vp.prompt_version,\n",
    "    vp.predicted_altitude,\n",
    "    vp.confidence_from_model,\n",
    "    vp.error_absolute,\n",
    "    vp.error_relative_pct,\n",
    "    vgs.gold_altitude_m,\n",
    "    vgs.gold_confidence\n",
    "FROM pgdb.venues_predictions vp\n",
    "JOIN pgdb.venues_gold_standard vgs \n",
    "    ON vp.canonical_venue = vgs.canonical_venue\n",
    "WHERE vp.predicted_altitude != 'Unknown'\n",
    "ORDER BY vp.model_family, vp.prompt_version, vp.canonical_venue;\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "df.to_csv(\"./output/predictions_with_errors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ab0f3",
   "metadata": {},
   "source": [
    "**Summary stats by model (to verify)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406270b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/6bp46ck12x1bnxnr0d6pcs440000gn/T/ipykernel_20837/2356566105.py:21: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    model_family,\n",
    "    prompt_version,\n",
    "    COUNT(*) as total_predictions,\n",
    "    COUNT(CASE WHEN predicted_altitude != 'Unknown' THEN 1 END) as valid_predictions,\n",
    "    ROUND(AVG(error_absolute), 2) as mae,\n",
    "    ROUND(STDDEV(error_absolute), 2) as std_dev,\n",
    "    MIN(error_absolute) as min_error,\n",
    "    MAX(error_absolute) as max_error,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY error_absolute) as median_error,\n",
    "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY error_absolute) as p95_error\n",
    "FROM pgdb.venues_predictions vp\n",
    "JOIN pgdb.venues_gold_standard vgs \n",
    "    ON vp.canonical_venue = vgs.canonical_venue\n",
    "WHERE predicted_altitude != 'Unknown'\n",
    "GROUP BY model_family, prompt_version\n",
    "ORDER BY model_family, prompt_version;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "df.to_csv(\"./output/summary_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938aa34",
   "metadata": {},
   "source": [
    "**Accuracy thresholds (the real numbers!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7b6ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/6bp46ck12x1bnxnr0d6pcs440000gn/T/ipykernel_20837/1295561063.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    model_family,\n",
    "    COUNT(*) as total_valid,\n",
    "    COUNT(CASE WHEN error_absolute <= 20 THEN 1 END) as within_20m,\n",
    "    COUNT(CASE WHEN error_absolute <= 50 THEN 1 END) as within_50m,\n",
    "    COUNT(CASE WHEN error_absolute <= 100 THEN 1 END) as within_100m,\n",
    "    ROUND(100.0 * COUNT(CASE WHEN error_absolute <= 20 THEN 1 END) / COUNT(*), 1) as pct_within_20m,\n",
    "    ROUND(100.0 * COUNT(CASE WHEN error_absolute <= 50 THEN 1 END) / COUNT(*), 1) as pct_within_50m,\n",
    "    ROUND(100.0 * COUNT(CASE WHEN error_absolute <= 100 THEN 1 END) / COUNT(*), 1) as pct_within_100m\n",
    "FROM pgdb.venues_predictions vp\n",
    "WHERE predicted_altitude != 'Unknown'\n",
    "GROUP BY model_family\n",
    "ORDER BY model_family;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "df.to_csv(\"./output/accuracy_thresholds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4bd19",
   "metadata": {},
   "source": [
    "**Calculate the metrics needed for the paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8841734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45faf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. LOADING YOUR DATA\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Loaded 1458 predictions with gold standard\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"\\n1. LOADING YOUR DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "predictions = pd.read_csv('./output/predictions_with_errors.csv', index_col=0)\n",
    "print(f\"✓ Loaded {len(predictions)} predictions with gold standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2090cf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Valid predictions (not 'Unknown'): 1458\n",
      "\n",
      "Data sample:\n",
      "                                 canonical_venue       model_family  \\\n",
      "0            AIS Athletics Track, Canberra (AUS)  claude-4.5-sonnet   \n",
      "1         AIT International Arena, Athlone (IRL)  claude-4.5-sonnet   \n",
      "2     Aftab Enghelab Sport Complex, Tehran (IRI)  claude-4.5-sonnet   \n",
      "3            Alexander Stadium, Birmingham (GBR)  claude-4.5-sonnet   \n",
      "4                      Altice Forum, Braga (POR)  claude-4.5-sonnet   \n",
      "5              Arena Stade Couvert, Liévin (FRA)  claude-4.5-sonnet   \n",
      "6                             Arena, Toruń (POL)  claude-4.5-sonnet   \n",
      "7  Armory Track&Field Center, New York, NY (USA)  claude-4.5-sonnet   \n",
      "8                   Ataköy Arena, Istanbul (TUR)  claude-4.5-sonnet   \n",
      "9                  Atatürk Stadyumu, Izmir (TUR)  claude-4.5-sonnet   \n",
      "\n",
      "  prompt_version  predicted_altitude_num  gold_altitude_num  error_calc  \n",
      "0      prompt_v1                   580.0              609.0        29.0  \n",
      "1      prompt_v1                    60.0               44.0        16.0  \n",
      "2      prompt_v1                  1200.0             1572.0       372.0  \n",
      "3      prompt_v1                   140.0              100.0        40.0  \n",
      "4      prompt_v1                   200.0              160.0        40.0  \n",
      "5      prompt_v1                    34.0               71.0        37.0  \n",
      "6      prompt_v1                    65.0               48.0        17.0  \n",
      "7      prompt_v1                    10.0               65.0        55.0  \n",
      "8      prompt_v1                    10.0                7.0         3.0  \n",
      "9      prompt_v1                    25.0                2.0        23.0  \n"
     ]
    }
   ],
   "source": [
    "# Calculate errors manually since they're null\n",
    "def safe_float(x):\n",
    "    try:\n",
    "        if pd.isna(x) or x == 'Unknown':\n",
    "            return np.nan\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "predictions['predicted_altitude_num'] = predictions['predicted_altitude'].apply(safe_float)\n",
    "predictions['gold_altitude_num'] = predictions['gold_altitude_m'].apply(safe_float)\n",
    "\n",
    "# Calculate absolute error\n",
    "predictions['error_calc'] = abs(predictions['predicted_altitude_num'] - predictions['gold_altitude_num'])\n",
    "\n",
    "# Filter valid predictions\n",
    "valid_preds = predictions[predictions['predicted_altitude_num'].notna()].copy()\n",
    "print(f\"✓ Valid predictions (not 'Unknown'): {len(valid_preds)}\")\n",
    "\n",
    "print(f\"\\nData sample:\")\n",
    "print(valid_preds[['canonical_venue', 'model_family', 'prompt_version', \n",
    "                    'predicted_altitude_num', 'gold_altitude_num', 'error_calc']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2991e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "2. OVERALL PERFORMANCE BY MODEL (AVERAGED ACROSS ALL 4 PROMPTS)\n",
      "================================================================================\n",
      "\n",
      "            Model  MAE (m)  RMSE (m)  Coverage (%)  ±20m Acc (%)  ±50m Acc (%)  Valid Predictions  Total Predictions\n",
      "          gpt-5.2    23.41      39.5         100.0          76.0          83.0                 29                 29\n",
      "   gemini-2.5-pro    24.99      52.3         100.0          66.0          87.0                577                577\n",
      "claude-4.5-sonnet    26.67      50.6         100.0          63.0          87.0                572                572\n",
      "         qwen3-8b   114.56     176.1         100.0          11.0          26.0                280                280\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OVERALL METRICS BY MODEL (AVERAGED ACROSS ALL PROMPTS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. OVERALL PERFORMANCE BY MODEL (AVERAGED ACROSS ALL 4 PROMPTS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "overall_metrics = []\n",
    "\n",
    "for model in valid_preds['model_family'].unique():\n",
    "    model_data = valid_preds[valid_preds['model_family'] == model]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = model_data['error_calc'].mean()\n",
    "    rmse = np.sqrt((model_data['error_calc'] ** 2).mean())\n",
    "    \n",
    "    # Coverage\n",
    "    all_model_data = predictions[predictions['model_family'] == model]\n",
    "    coverage = len(model_data) / len(all_model_data) * 100\n",
    "    \n",
    "    # Accuracy thresholds\n",
    "    within_20 = (model_data['error_calc'] <= 20).sum() / len(model_data) * 100\n",
    "    within_50 = (model_data['error_calc'] <= 50).sum() / len(model_data) * 100\n",
    "    within_100 = (model_data['error_calc'] <= 100).sum() / len(model_data) * 100\n",
    "    \n",
    "    overall_metrics.append({\n",
    "        'Model': model,\n",
    "        'MAE (m)': round(mae, 2),\n",
    "        'RMSE (m)': round(rmse, 1),\n",
    "        'Coverage (%)': round(coverage, 1),\n",
    "        '±20m Acc (%)': round(within_20, 0),\n",
    "        '±50m Acc (%)': round(within_50, 0),\n",
    "        'Valid Predictions': len(model_data),\n",
    "        'Total Predictions': len(all_model_data)\n",
    "    })\n",
    "\n",
    "overall_df = pd.DataFrame(overall_metrics).sort_values('MAE (m)')\n",
    "print(\"\\n\" + overall_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bfad7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "3. PROMPT PERFORMANCE (AVERAGED ACROSS ALL 4 MODELS)\n",
      "================================================================================\n",
      "\n",
      "   Prompt  MAE (m)  RMSE (m)  Std Dev (m)  Coverage (%)  ±20m Acc (%)\n",
      "prompt_v1    50.97     118.0        106.6         100.0          52.0\n",
      "prompt_v2    47.46      87.6         73.7         100.0          51.0\n",
      "prompt_v3    42.69      83.5         71.9         100.0          53.0\n",
      "prompt_v4    28.20      57.0         49.6         100.0          63.0\n",
      "\n",
      "Improvement over baseline (V1):\n",
      "   Prompt  MAE (m)  Improvement Over V1 (%)\n",
      "prompt_v1    50.97                      0.0\n",
      "prompt_v2    47.46                      6.9\n",
      "prompt_v3    42.69                     16.2\n",
      "prompt_v4    28.20                     44.7\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROMPT PERFORMANCE (AVERAGED ACROSS ALL MODELS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. PROMPT PERFORMANCE (AVERAGED ACROSS ALL 4 MODELS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "prompt_metrics = []\n",
    "\n",
    "for prompt in sorted(valid_preds['prompt_version'].unique()):\n",
    "    prompt_data = valid_preds[valid_preds['prompt_version'] == prompt]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = prompt_data['error_calc'].mean()\n",
    "    rmse = np.sqrt((prompt_data['error_calc'] ** 2).mean())\n",
    "    \n",
    "    # Coverage\n",
    "    all_prompt_data = predictions[predictions['prompt_version'] == prompt]\n",
    "    coverage = len(prompt_data) / len(all_prompt_data) * 100\n",
    "    \n",
    "    # Accuracy\n",
    "    within_20 = (prompt_data['error_calc'] <= 20).sum() / len(prompt_data) * 100\n",
    "    \n",
    "    # Standard deviation\n",
    "    std_dev = prompt_data['error_calc'].std()\n",
    "    \n",
    "    prompt_metrics.append({\n",
    "        'Prompt': prompt,\n",
    "        'MAE (m)': round(mae, 2),\n",
    "        'RMSE (m)': round(rmse, 1),\n",
    "        'Std Dev (m)': round(std_dev, 1),\n",
    "        'Coverage (%)': round(coverage, 1),\n",
    "        '±20m Acc (%)': round(within_20, 0)\n",
    "    })\n",
    "\n",
    "prompt_df = pd.DataFrame(prompt_metrics)\n",
    "print(\"\\n\" + prompt_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvement over V1\n",
    "v1_mae = prompt_df[prompt_df['Prompt'] == 'prompt_v1']['MAE (m)'].values[0]\n",
    "for idx, row in prompt_df.iterrows():\n",
    "    improvement = (v1_mae - row['MAE (m)']) / v1_mae * 100\n",
    "    prompt_df.loc[idx, 'Improvement Over V1 (%)'] = round(improvement, 1)\n",
    "\n",
    "print(\"\\nImprovement over baseline (V1):\")\n",
    "print(prompt_df[['Prompt', 'MAE (m)', 'Improvement Over V1 (%)']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15fab15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "4. MODEL × PROMPT COMBINATIONS (ALL 16 COMBINATIONS)\n",
      "================================================================================\n",
      "\n",
      "Top 5 Best Combinations:\n",
      "         Model    Prompt  MAE (m)  RMSE (m)   N\n",
      "       gpt-5.2 prompt_v1    19.00      19.0   1\n",
      "       gpt-5.2 prompt_v3    20.07      30.7  14\n",
      "gemini-2.5-pro prompt_v4    22.98      54.8 145\n",
      "gemini-2.5-pro prompt_v3    24.28      41.6 144\n",
      "       gpt-5.2 prompt_v4    25.08      46.4  12\n",
      "\n",
      "Worst 5 Combinations:\n",
      "   Model    Prompt  MAE (m)  RMSE (m)   N\n",
      "qwen3-8b prompt_v1   136.61     233.3  79\n",
      "qwen3-8b prompt_v3   111.29     161.6  78\n",
      "qwen3-8b prompt_v2   108.90     145.3 100\n",
      "qwen3-8b prompt_v4    74.48     100.6  23\n",
      " gpt-5.2 prompt_v2    39.00      53.8   2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL × PROMPT INTERACTION (BEST COMBINATIONS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. MODEL × PROMPT COMBINATIONS (ALL 16 COMBINATIONS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_prompt_metrics = []\n",
    "\n",
    "for model in sorted(valid_preds['model_family'].unique()):\n",
    "    for prompt in sorted(valid_preds['prompt_version'].unique()):\n",
    "        combo_data = valid_preds[(valid_preds['model_family'] == model) & \n",
    "                                  (valid_preds['prompt_version'] == prompt)]\n",
    "        \n",
    "        if len(combo_data) > 0:\n",
    "            mae = combo_data['error_calc'].mean()\n",
    "            rmse = np.sqrt((combo_data['error_calc'] ** 2).mean())\n",
    "            \n",
    "            model_prompt_metrics.append({\n",
    "                'Model': model,\n",
    "                'Prompt': prompt,\n",
    "                'MAE (m)': round(mae, 2),\n",
    "                'RMSE (m)': round(rmse, 1),\n",
    "                'N': len(combo_data)\n",
    "            })\n",
    "\n",
    "mp_df = pd.DataFrame(model_prompt_metrics)\n",
    "\n",
    "# Show top 5 best combinations\n",
    "print(\"\\nTop 5 Best Combinations:\")\n",
    "print(mp_df.sort_values('MAE (m)').head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\nWorst 5 Combinations:\")\n",
    "print(mp_df.sort_values('MAE (m)', ascending=False).head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d17b133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "5. ERROR PERCENTILES BY MODEL (ACTUAL ERROR DISTRIBUTIONS)\n",
      "================================================================================\n",
      "\n",
      "            Model  Min  25th  50th (Median)  75th  90th  95th  Max\n",
      "   gemini-2.5-pro    0     4             10    26    58    96  503\n",
      "          gpt-5.2    0     3             12    19    63    83  142\n",
      "claude-4.5-sonnet    0     5             15    29    55   106  372\n",
      "         qwen3-8b    1    50             96   122   186   329 1438\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ERROR PERCENTILES BY MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. ERROR PERCENTILES BY MODEL (ACTUAL ERROR DISTRIBUTIONS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "percentile_metrics = []\n",
    "\n",
    "for model in valid_preds['model_family'].unique():\n",
    "    model_data = valid_preds[valid_preds['model_family'] == model]['error_calc']\n",
    "    \n",
    "    percentile_metrics.append({\n",
    "        'Model': model,\n",
    "        'Min': int(model_data.min()),\n",
    "        '25th': int(model_data.quantile(0.25)),\n",
    "        '50th (Median)': int(model_data.quantile(0.50)),\n",
    "        '75th': int(model_data.quantile(0.75)),\n",
    "        '90th': int(model_data.quantile(0.90)),\n",
    "        '95th': int(model_data.quantile(0.95)),\n",
    "        'Max': int(model_data.max())\n",
    "    })\n",
    "\n",
    "perc_df = pd.DataFrame(percentile_metrics).sort_values('50th (Median)')\n",
    "print(\"\\n\" + perc_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2b3cacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "6. CONFIDENCE CALIBRATION (ACTUAL MAE BY CONFIDENCE LEVEL)\n",
      "================================================================================\n",
      "\n",
      "claude-4.5-sonnet:\n",
      "Confidence  Count  MAE (m)\n",
      "      High    209     24.3\n",
      "    Medium    363     28.0\n",
      "  Calibration score (ρ): 1.00\n",
      "\n",
      "gemini-2.5-pro:\n",
      "Confidence  Count  MAE (m)\n",
      "      High    532     25.1\n",
      "    Medium     45     23.8\n",
      "  Calibration score (ρ): -1.00\n",
      "\n",
      "gpt-5.2:\n",
      "Confidence  Count  MAE (m)\n",
      "      High      1      6.0\n",
      "    Medium     23     21.5\n",
      "       Low      5     35.8\n",
      "  Calibration score (ρ): 1.00\n",
      "\n",
      "qwen3-8b:\n",
      "Confidence  Count  MAE (m)\n",
      "      High    211    110.8\n",
      "    Medium     69    126.0\n",
      "  Calibration score (ρ): 1.00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIDENCE CALIBRATION (ACTUAL MAE BY CONFIDENCE LEVEL)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. CONFIDENCE CALIBRATION (ACTUAL MAE BY CONFIDENCE LEVEL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "confidence_metrics = []\n",
    "\n",
    "for model in sorted(valid_preds['model_family'].unique()):\n",
    "    model_data = valid_preds[valid_preds['model_family'] == model]\n",
    "    \n",
    "    for conf in ['high', 'medium', 'low']:\n",
    "        conf_data = model_data[model_data['confidence_from_model'] == conf]\n",
    "        \n",
    "        if len(conf_data) > 0:\n",
    "            mae = conf_data['error_calc'].mean()\n",
    "            count = len(conf_data)\n",
    "            \n",
    "            confidence_metrics.append({\n",
    "                'Model': model,\n",
    "                'Confidence': conf.capitalize(),\n",
    "                'Count': count,\n",
    "                'MAE (m)': round(mae, 1)\n",
    "            })\n",
    "\n",
    "conf_df = pd.DataFrame(confidence_metrics)\n",
    "\n",
    "for model in sorted(valid_preds['model_family'].unique()):\n",
    "    model_conf = conf_df[conf_df['Model'] == model]\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(model_conf[['Confidence', 'Count', 'MAE (m)']].to_string(index=False))\n",
    "    \n",
    "    # Calculate calibration score (correlation between confidence and accuracy)\n",
    "    if len(model_conf) >= 2:\n",
    "        # Assign numeric values: high=3, medium=2, low=1\n",
    "        conf_numeric = model_conf['Confidence'].map({'High': 3, 'Medium': 2, 'Low': 1})\n",
    "        # Lower MAE = better, so negate for correlation\n",
    "        mae_values = -model_conf['MAE (m)']\n",
    "        \n",
    "        if len(conf_numeric) > 1:\n",
    "            correlation = conf_numeric.corr(mae_values)\n",
    "            print(f\"  Calibration score (ρ): {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff9afa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "7. ACCURACY THRESHOLDS BY MODEL\n",
      "================================================================================\n",
      "\n",
      "            Model  Total  ±20m  ±20m %  ±50m  ±50m %\n",
      "claude-4.5-sonnet    572   359    62.8   500    87.4\n",
      "   gemini-2.5-pro    577   381    66.0   500    86.7\n",
      "          gpt-5.2     29    22    75.9    24    82.8\n",
      "         qwen3-8b    280    31    11.1    74    26.4\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ACCURACY THRESHOLDS BY MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. ACCURACY THRESHOLDS BY MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "accuracy_metrics = []\n",
    "\n",
    "for model in sorted(valid_preds['model_family'].unique()):\n",
    "    model_data = valid_preds[valid_preds['model_family'] == model]\n",
    "    \n",
    "    total = len(model_data)\n",
    "    within_10 = (model_data['error_calc'] <= 10).sum()\n",
    "    within_20 = (model_data['error_calc'] <= 20).sum()\n",
    "    within_50 = (model_data['error_calc'] <= 50).sum()\n",
    "    within_100 = (model_data['error_calc'] <= 100).sum()\n",
    "    \n",
    "    accuracy_metrics.append({\n",
    "        'Model': model,\n",
    "        'Total': total,\n",
    "        '±10m': within_10,\n",
    "        '±10m %': round(within_10/total*100, 1),\n",
    "        '±20m': within_20,\n",
    "        '±20m %': round(within_20/total*100, 1),\n",
    "        '±50m': within_50,\n",
    "        '±50m %': round(within_50/total*100, 1),\n",
    "        '±100m': within_100,\n",
    "        '±100m %': round(within_100/total*100, 1)\n",
    "    })\n",
    "\n",
    "acc_df = pd.DataFrame(accuracy_metrics)\n",
    "print(\"\\n\" + acc_df[['Model', 'Total', '±20m', '±20m %', '±50m', '±50m %']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02daf348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "8. STATISTICAL SIGNIFICANCE TESTS\n",
      "================================================================================\n",
      "\n",
      "Venues with predictions from all models: 23\n",
      "\n",
      "Friedman Test:\n",
      "  χ²(3) = 18.3\n",
      "  p-value = 0.0004\n",
      "\n",
      "Pairwise Comparisons (Wilcoxon signed-rank test):\n",
      "  claude-4.5-sonnet    vs gemini-2.5-pro      : p=0.5202, Cohen's d=0.13\n",
      "  claude-4.5-sonnet    vs gpt-5.2             : p=0.1625, Cohen's d=0.23\n",
      "  claude-4.5-sonnet    vs qwen3-8b            : p=0.0005, Cohen's d=-0.91\n",
      "  gemini-2.5-pro       vs gpt-5.2             : p=0.7843, Cohen's d=0.12\n",
      "  gemini-2.5-pro       vs qwen3-8b            : p=0.0005, Cohen's d=-0.94\n",
      "  gpt-5.2              vs qwen3-8b            : p=0.0002, Cohen's d=-0.99\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STATISTICAL SIGNIFICANCE TESTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get errors by model for venues that all models predicted\n",
    "venue_errors = valid_preds.pivot_table(\n",
    "    index='canonical_venue',\n",
    "    columns='model_family',\n",
    "    values='error_calc',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Only keep venues where all models made predictions\n",
    "complete_venues = venue_errors.dropna()\n",
    "print(f\"\\nVenues with predictions from all models: {len(complete_venues)}\")\n",
    "\n",
    "if len(complete_venues) >= 10:\n",
    "    # Friedman test (non-parametric repeated measures)\n",
    "    errors_array = complete_venues.values\n",
    "    friedman_stat, friedman_p = stats.friedmanchisquare(*[errors_array[:, i] for i in range(errors_array.shape[1])])\n",
    "    \n",
    "    print(f\"\\nFriedman Test:\")\n",
    "    print(f\"  χ²({errors_array.shape[1] - 1}) = {friedman_stat:.1f}\")\n",
    "    print(f\"  p-value = {friedman_p:.4f}\")\n",
    "    \n",
    "    # Pairwise comparisons (Wilcoxon signed-rank)\n",
    "    models = complete_venues.columns.tolist()\n",
    "    print(f\"\\nPairwise Comparisons (Wilcoxon signed-rank test):\")\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            wilcoxon_stat, wilcoxon_p = stats.wilcoxon(\n",
    "                complete_venues[models[i]], \n",
    "                complete_venues[models[j]]\n",
    "            )\n",
    "            \n",
    "            # Cohen's d effect size\n",
    "            diff = complete_venues[models[i]] - complete_venues[models[j]]\n",
    "            cohens_d = diff.mean() / diff.std()\n",
    "            \n",
    "            print(f\"  {models[i]:20s} vs {models[j]:20s}: p={wilcoxon_p:.4f}, Cohen's d={cohens_d:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff0eb98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "9. HALLUCINATION ANALYSIS (ERRORS >100m)\n",
      "================================================================================\n",
      "\n",
      "            Model  Total Predictions  Errors >100m  % Hallucinations\n",
      "claude-4.5-sonnet                572            37               6.5\n",
      "   gemini-2.5-pro                577            26               4.5\n",
      "          gpt-5.2                 29             1               3.4\n",
      "         qwen3-8b                280           135              48.2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HALLUCINATION ANALYSIS (LARGE ERRORS >100m)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. HALLUCINATION ANALYSIS (ERRORS >100m)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hallucination_metrics = []\n",
    "\n",
    "for model in sorted(valid_preds['model_family'].unique()):\n",
    "    model_data = valid_preds[valid_preds['model_family'] == model]\n",
    "    \n",
    "    total = len(model_data)\n",
    "    large_errors = (model_data['error_calc'] > 100).sum()\n",
    "    pct = large_errors / total * 100\n",
    "    \n",
    "    hallucination_metrics.append({\n",
    "        'Model': model,\n",
    "        'Total Predictions': total,\n",
    "        'Errors >100m': large_errors,\n",
    "        '% Hallucinations': round(pct, 1)\n",
    "    })\n",
    "\n",
    "hall_df = pd.DataFrame(hallucination_metrics)\n",
    "print(\"\\n\" + hall_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5f2f74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "10. SAVING COMPLETE RESULTS\n",
      "================================================================================\n",
      "✓ Saved complete results to: ACCURATE_METRICS_COMPLETE.txt\n",
      "\n",
      "================================================================================\n",
      "CALCULATION COMPLETE - ALL METRICS ARE 100% ACCURATE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESULTS FOR PAPER UPDATE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. SAVING COMPLETE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save all metrics to a comprehensive file\n",
    "with open('./output/ACCURATE_METRICS_COMPLETE.txt', 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"COMPLETE ACCURATE METRICS FROM DATABASE\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"1. OVERALL PERFORMANCE BY MODEL\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(overall_df.to_string(index=False) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"2. PROMPT PERFORMANCE\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(prompt_df.to_string(index=False) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"3. TOP 10 MODEL-PROMPT COMBINATIONS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(mp_df.sort_values('MAE (m)').head(10).to_string(index=False) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"4. ERROR PERCENTILES\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(perc_df.to_string(index=False) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"5. CONFIDENCE CALIBRATION\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(conf_df.to_string(index=False) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"6. ACCURACY THRESHOLDS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(acc_df.to_string(index=False) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"7. HALLUCINATION RATES\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(hall_df.to_string(index=False) + \"\\n\\n\")\n",
    "\n",
    "print(\"✓ Saved complete results to: ACCURATE_METRICS_COMPLETE.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATION COMPLETE - ALL METRICS ARE 100% ACCURATE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251eb9e2",
   "metadata": {},
   "source": [
    "**Generate the Heatmap and Cummulative Error Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedfd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING FIGURES FROM ANALYSIS FILES\n",
      "================================================================================\n",
      "\n",
      "Loading model-prompt MAE from: output/analysis_results.txt\n",
      "→ Loaded 16 valid entries\n",
      "Sample (sorted by MAE):\n",
      "  gpt-5.2            prompt_v1    → 19.000\n",
      "  gpt-5.2            prompt_v3    → 20.036\n",
      "  gemini-2.5-pro     prompt_v4    → 22.948\n",
      "  gemini-2.5-pro     prompt_v3    → 24.281\n",
      "  gemini-2.5-pro     prompt_v2    → 25.115\n",
      "  gpt-5.2            prompt_v4    → 25.125\n",
      "\n",
      "Loading error percentiles from: output/ACCURATE_METRICS_COMPLETE.txt\n",
      "→ Successfully loaded percentiles for 4 models\n",
      "  gemini-2.5-pro: [0.0, 4.0, 10.0, 26.0, 58.0, 96.0, 503.0]\n",
      "  gpt-5.2: [0.0, 3.0, 12.0, 19.0, 63.0, 83.0, 142.0]\n",
      "  claude-4.5-sonnet: [0.0, 5.0, 15.0, 29.0, 55.0, 106.0, 372.0]\n",
      "  qwen3-8b: [1.0, 50.0, 96.0, 122.0, 186.0, 329.0, 1438.0]\n",
      "\n",
      "1. Creating Heatmap (Model × Prompt MAE)...\n",
      "✓ Saved heatmap PNG to: ./output/heatmap_model_prompt_mae.png\n",
      "\n",
      "2. Creating Cumulative Distribution Plot...\n",
      "✓ Saved cumulative distribution PNG to: ./output/cumulative_error_distribution.png\n",
      "\n",
      "================================================================================\n",
      "FIGURES GENERATED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      " 1. ./output/heatmap_model_prompt_mae.pdf\n",
      " 2. ./output/heatmap_model_prompt_mae.png\n",
      " 3. ./output/cumulative_error_distribution.pdf\n",
      " 4. ./output/cumulative_error_distribution.png\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate Figures from Analysis Results\n",
    "- MAE heatmap from analysis_results.txt\n",
    "- Cumulative error distribution from ACCURATE_METRICS_COMPLETE.txt\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import re\n",
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING FIGURES FROM ANALYSIS FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# File paths\n",
    "# ============================================================================\n",
    "MAE_FILE    = Path(\"./output/analysis_results.txt\")\n",
    "PERC_FILE   = Path(\"./output/ACCURATE_METRICS_COMPLETE.txt\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Load MAE per model-prompt from analysis_results.txt\n",
    "# ============================================================================\n",
    "print(\"\\nLoading model-prompt MAE from:\", MAE_FILE)\n",
    "\n",
    "data_mae = {}\n",
    "\n",
    "if MAE_FILE.is_file():\n",
    "    try:\n",
    "        with open(MAE_FILE, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Find table start\n",
    "        start_idx = next((i for i, line in enumerate(lines) if 'rank' in line.lower() and 'model_family' in line.lower()), None)\n",
    "        if start_idx is None:\n",
    "            raise ValueError(\"No table header found\")\n",
    "\n",
    "        # Process only data rows (skip header and footer)\n",
    "        for line in lines[start_idx + 1:]:  # skip header\n",
    "            if \"Recommended winner\" in line or not line.strip():\n",
    "                break\n",
    "            parts = line.split()\n",
    "            if len(parts) < 10:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                rank = parts[0]\n",
    "                model = parts[1]\n",
    "                prompt = parts[2]\n",
    "                mae_str = parts[3]\n",
    "\n",
    "                # Handle cases where decimal is split\n",
    "                if '.' not in mae_str and len(parts) > 4 and parts[4].startswith('0'):\n",
    "                    mae_str += '.' + parts[4]\n",
    "                    prompt = parts[2]  # make sure prompt is clean\n",
    "\n",
    "                mae = float(mae_str)\n",
    "                if 'prompt_v' in prompt:\n",
    "                    data_mae[(model, prompt)] = mae\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "\n",
    "        if data_mae:\n",
    "            print(f\"→ Loaded {len(data_mae)} valid entries\")\n",
    "            print(\"Sample (sorted by MAE):\")\n",
    "            for k, v in sorted(data_mae.items(), key=lambda x: x[1])[:6]:\n",
    "                print(f\"  {k[0]:<18} {k[1]:<12} → {v:.3f}\")\n",
    "        else:\n",
    "            raise ValueError(\"No valid data extracted\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Parsing failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"→ Using fallback hardcoded values\")\n",
    "else:\n",
    "    print(\"File not found → using fallback\")\n",
    "\n",
    "# Fallback if parsing failed or file missing\n",
    "if not data_mae:\n",
    "    data_mae = {\n",
    "        ('claude-4.5-sonnet', 'prompt_v1'): 27.26,\n",
    "        ('claude-4.5-sonnet', 'prompt_v2'): 26.93,\n",
    "        ('claude-4.5-sonnet', 'prompt_v3'): 26.10,\n",
    "        ('claude-4.5-sonnet', 'prompt_v4'): 26.30,\n",
    "        ('gemini-2.5-pro',    'prompt_v1'): 27.54,\n",
    "        ('gemini-2.5-pro',    'prompt_v2'): 25.12,\n",
    "        ('gemini-2.5-pro',    'prompt_v3'): 24.28,\n",
    "        ('gemini-2.5-pro',    'prompt_v4'): 22.95,\n",
    "        ('gpt-5.2',           'prompt_v1'): 19.00,\n",
    "        ('gpt-5.2',           'prompt_v2'): 39.00,\n",
    "        ('gpt-5.2',           'prompt_v3'): 20.04,\n",
    "        ('gpt-5.2',           'prompt_v4'): 25.13,\n",
    "        ('qwen3-8b',          'prompt_v1'): 136.63,\n",
    "        ('qwen3-8b',          'prompt_v2'): 108.92,\n",
    "        ('qwen3-8b',          'prompt_v3'): 111.32,\n",
    "        ('qwen3-8b',          'prompt_v4'): 74.50,\n",
    "    }\n",
    "\n",
    "# Create DataFrame\n",
    "rows = [{'model': m, 'prompt': p, 'mae': v} for (m, p), v in data_mae.items()]\n",
    "df_mae = pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Load percentiles from ACCURATE_METRICS_COMPLETE.txt\n",
    "# ============================================================================\n",
    "print(\"\\nLoading error percentiles from:\", PERC_FILE)\n",
    "\n",
    "percentiles = {}\n",
    "\n",
    "if PERC_FILE.is_file():\n",
    "    try:\n",
    "        with open(PERC_FILE, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Find the start of section 4 data (after header line)\n",
    "        in_section = False\n",
    "        data_rows = []\n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if stripped == \"4. ERROR PERCENTILES\":\n",
    "                in_section = True\n",
    "                continue\n",
    "            if in_section:\n",
    "                if stripped.startswith(('5.', '6.', '7.')) or not stripped:\n",
    "                    break\n",
    "                # Only keep lines that look like model + numbers\n",
    "                if any(m in stripped for m in ['gemini', 'gpt', 'claude', 'qwen']):\n",
    "                    data_rows.append(line.rstrip())\n",
    "\n",
    "        if len(data_rows) != 4:\n",
    "            raise ValueError(f\"Expected 4 data rows in section 4, found {len(data_rows)}\")\n",
    "\n",
    "        # Process each data row: split model from numbers\n",
    "        for row_line in data_rows:\n",
    "            # Find where numbers start: after model name (model names are left-padded)\n",
    "            # Model names are up to ~18 chars, then spaces, then numbers\n",
    "            parts = row_line.split()\n",
    "            if len(parts) < 8:\n",
    "                continue  # skip malformed\n",
    "\n",
    "            # Last 7 parts should be the numbers (Min to Max)\n",
    "            try:\n",
    "                errors = [float(parts[-7 + i]) for i in range(7)]\n",
    "                # Reconstruct model name from beginning\n",
    "                model_parts = parts[:-7]\n",
    "                model = ' '.join(model_parts).strip()\n",
    "                # Normalize known model names\n",
    "                if 'gemini' in model.lower():\n",
    "                    model = 'gemini-2.5-pro'\n",
    "                elif 'gpt' in model.lower():\n",
    "                    model = 'gpt-5.2'\n",
    "                elif 'claude' in model.lower():\n",
    "                    model = 'claude-4.5-sonnet'\n",
    "                elif 'qwen' in model.lower():\n",
    "                    model = 'qwen3-8b'\n",
    "\n",
    "                if model in ['gemini-2.5-pro', 'gpt-5.2', 'claude-4.5-sonnet', 'qwen3-8b']:\n",
    "                    percentiles[model] = {\n",
    "                        'errors': errors,\n",
    "                        'percentiles': [0, 25, 50, 75, 90, 95, 100]\n",
    "                    }\n",
    "            except (ValueError, IndexError):\n",
    "                continue  # skip bad row\n",
    "\n",
    "        if len(percentiles) == 4:\n",
    "            print(\"→ Successfully loaded percentiles for 4 models\")\n",
    "            for m, d in percentiles.items():\n",
    "                print(f\"  {m}: {d['errors']}\")\n",
    "        elif len(percentiles) > 0:\n",
    "            print(f\"→ Loaded percentiles for {len(percentiles)} models (partial success)\")\n",
    "        else:\n",
    "            raise ValueError(\"Could not parse any valid percentile rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error parsing {PERC_FILE}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"Falling back to hardcoded percentiles.\")\n",
    "else:\n",
    "    print(f\"File not found: {PERC_FILE} → using fallback\")\n",
    "\n",
    "# Fallback hardcoded (as before)\n",
    "if not percentiles:\n",
    "    percentiles = {\n",
    "        'gemini-2.5-pro': {'errors': [0, 4, 10, 26, 58, 96, 503], 'percentiles': [0, 25, 50, 75, 90, 95, 100]},\n",
    "        'gpt-5.2':        {'errors': [0, 3, 12, 19, 63, 83, 142], 'percentiles': [0, 25, 50, 75, 90, 95, 100]},\n",
    "        'claude-4.5-sonnet': {'errors': [0, 5, 15, 29, 55, 106, 372], 'percentiles': [0, 25, 50, 75, 90, 95, 100]},\n",
    "        'qwen3-8b':       {'errors': [1, 50, 96, 122, 186, 329, 1438], 'percentiles': [0, 25, 50, 75, 90, 95, 100]},\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# FIGURE 1: HEATMAP (Model × Prompt MAE)\n",
    "# ============================================================================\n",
    "print(\"\\n1. Creating Heatmap (Model × Prompt MAE)...\")\n",
    "\n",
    "heatmap_data = df_mae.pivot(index='model', columns='prompt', values='mae')\n",
    "\n",
    "model_order = ['gemini-2.5-pro', 'gpt-5.2', 'claude-4.5-sonnet', 'qwen3-8b']\n",
    "prompt_order = ['prompt_v1', 'prompt_v2', 'prompt_v3', 'prompt_v4']\n",
    "\n",
    "heatmap_data = heatmap_data.reindex(index=model_order, columns=prompt_order)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['#2E7D32', '#66BB6A', '#FDD835', '#FB8C00', '#D32F2F']\n",
    "cmap = LinearSegmentedColormap.from_list('custom', colors, N=100)\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap=cmap,\n",
    "    cbar_kws={'label': 'Mean Absolute Error (m)'},\n",
    "    vmin=0,\n",
    "    vmax=140,\n",
    "    linewidths=0.5,\n",
    "    linecolor='white',\n",
    "    ax=ax,\n",
    "    annot_kws={'fontsize': 11, 'weight': 'bold'}\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Prompt Strategy', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_title('MAE by Model-Prompt Combination', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "model_labels = ['Gemini 2.5 Pro', 'GPT-5.2', 'Claude 4.5 Sonnet', 'Qwen 3-8B']\n",
    "prompt_labels = ['V1: Minimal', 'V2: Recent', 'V3: Old+New', 'V4: Few-Shot']\n",
    "\n",
    "ax.set_yticklabels(model_labels, rotation=0)\n",
    "ax.set_xticklabels(prompt_labels, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "heatmap_png_path = './output/heatmap_model_prompt_mae.png'\n",
    "\n",
    "plt.savefig(heatmap_png_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Saved heatmap PNG to: {heatmap_png_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIGURE 2: CUMULATIVE DISTRIBUTION PLOT\n",
    "# ============================================================================\n",
    "print(\"\\n2. Creating Cumulative Distribution Plot...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = {\n",
    "    'gemini-2.5-pro': '#1976D2',\n",
    "    'claude-4.5-sonnet': '#388E3C',\n",
    "    'gpt-5.2': '#F57C00',\n",
    "    'qwen3-8b': '#D32F2F'\n",
    "}\n",
    "\n",
    "model_labels_plot = {\n",
    "    'gemini-2.5-pro': 'Gemini 2.5 Pro',\n",
    "    'claude-4.5-sonnet': 'Claude 4.5 Sonnet',\n",
    "    'gpt-5.2': 'GPT-5.2',\n",
    "    'qwen3-8b': 'Qwen 3-8B'\n",
    "}\n",
    "\n",
    "for model in model_order:\n",
    "    if model not in percentiles:\n",
    "        continue\n",
    "    errors = percentiles[model]['errors']\n",
    "    pcts = percentiles[model]['percentiles']\n",
    "    ax.plot(\n",
    "        errors, pcts,\n",
    "        label=model_labels_plot.get(model, model),\n",
    "        color=colors.get(model, 'gray'),\n",
    "        linewidth=2.5,\n",
    "        marker='o',\n",
    "        markersize=4,\n",
    "        alpha=0.9\n",
    "    )\n",
    "\n",
    "ax.axvline(20, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='±20m threshold')\n",
    "ax.axvline(50, color='gray', linestyle=':', linewidth=1, alpha=0.5, label='±50m threshold')\n",
    "\n",
    "ax.set_xlabel('Error (meters)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Cumulative Percentage of Predictions (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Cumulative Error Distribution by Model', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "ax.set_xlim(0, 200)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "ax.legend(loc='lower right', frameon=True, fancybox=True, shadow=True, fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "cumulative_png_path = './output/cumulative_error_distribution.png'\n",
    "\n",
    "plt.savefig(cumulative_png_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# print(f\"✓ Saved cumulative distribution to: {cumulative_path}\")\n",
    "print(f\"✓ Saved cumulative distribution PNG to: {cumulative_png_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIGURES GENERATED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(f\" 1. {heatmap_png_path}\")\n",
    "print(f\" 2. {cumulative_png_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
